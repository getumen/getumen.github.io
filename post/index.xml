<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 雑記</title>
    <link>https://getumen.github.io/post/</link>
    <description>Recent content in Posts on 雑記</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Thu, 03 Jan 2019 00:19:02 +0900</lastBuildDate>
    
	<atom:link href="https://getumen.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems ２章 (1)</title>
      <link>https://getumen.github.io/2019/01/03/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%92%E7%AB%A0-1/</link>
      <pubDate>Thu, 03 Jan 2019 00:19:02 +0900</pubDate>
      
      <guid>https://getumen.github.io/2019/01/03/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%92%E7%AB%A0-1/</guid>
      <description>&lt;h1 id=&#34;２章-stochastic-bandits-fundamental-results&#34;&gt;２章 Stochastic Bandits: Fundamental Results&lt;/h1&gt;

&lt;h2 id=&#34;確率的バンディット問題とは&#34;&gt;確率的バンディット問題とは&lt;/h2&gt;

&lt;p&gt;各アームを$i=1,\dots,K$とし，アームに対応する未知の確率分布を$\nu_i$とする．
各ステップ$t=1,2,\dots$で，予測者はアーム$I_{t}\in{1,\dots,K}$を選び，$\nu_{I_t}$から（過去とは独立に）報酬$X_{I_t,t}$を受け取る．
アーム$i$の平均を$\mu_i$とし，アームの平均の最大値と，最大値を取るアームのインデックスをそれぞれ$\mu^*=\max_{i=1,\dots,K}\mu_i$と$i^*\in\max_{i=1,\dots,K}\mu_i$とする．
確率的設定では擬リグレットを主に考える．
これは，確率的設定においては実際の報酬について最適な行動と比較するより，最適な行動の期待値と比較するほうが自然であるためだ．&lt;/p&gt;

&lt;p&gt;以下のように擬リグレットを書き換えた定式化も用いる．
最初から$s$ラウンドまでのプレイヤーに選ばれたアーム$i$の回数を$T_i(s)=\sum_{t=1}^s\mathbb{1}_{I_t=i}$とする．
アーム$i$のsuboptimalityパラメータを$\Delta_i=\mu^*-\mu_i$とする．
擬リグレットは$$\overline{R}_n=\left(\sum_{i=1}^K\mathbb{E}T_i(n)\right)\mu^*-\sum_{i=1}^K\mathbb{E}T_i(n)\mu_i=\sum_{i=1}^K\Delta_i\mathbb{E}T_i(n)$$と書くことができる．&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems １章 (2)</title>
      <link>https://getumen.github.io/2019/01/02/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%91%E7%AB%A0-2/</link>
      <pubDate>Wed, 02 Jan 2019 21:34:59 +0900</pubDate>
      
      <guid>https://getumen.github.io/2019/01/02/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%91%E7%AB%A0-2/</guid>
      <description>&lt;h1 id=&#34;１章-導入&#34;&gt;１章 導入&lt;/h1&gt;

&lt;h2 id=&#34;確率的バンディット問題の設定とは&#34;&gt;確率的バンディット問題の設定とは&lt;/h2&gt;

&lt;p&gt;それぞれのアームを$i=1,\dots,K$とする．
対応する$[0,1]$上の未知の確率分布を$\nu_i$とする．
選択したアームに対応するが確率分布$\nu_i$から独立にサンプリングされた報酬を$X_{i,t}$とする．&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;確率的バンディット問題&lt;/em&gt;&lt;br /&gt;
既知のパラメータ：アーム数$K$，取りうるラウンド数$n\ge K$&lt;br /&gt;
未知のパラメータ：$[0,1]$上の$K$個の確率分布$\nu_1,\nu_2,\dots,\nu_K$&lt;br /&gt;
各ラウンドに対して，&lt;br /&gt;
1. 予測者はアーム$I_t\in\{1,\dots,K\}$を選ぶ
2. 与えられた選択$I_t$に対して，過去に依存しない報酬$X_{I_t,t}\sim\nu_{I_t}$が環境から支払われ，予測者に明らかになる．&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$i=1,\dots,K$に対して，$\nu_i$の平均（アーム$i$の報酬の平均）を$\mu_i$と記す．
また，平均が最大のアームの平均値とそのアームのインデックスを$\mu^*=\max_{i=1,\dots,K}\mu_i$と$i^*=\rm arg\max_{i=1,\dots,K}\mu_i$とする．
擬リグレットは$$\overline{R}_n=n\mu^*-\sum_{t=1}^n\mathbb{E}[\mu_{I_t}]$$
とかける．&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems １章 (1)</title>
      <link>https://getumen.github.io/2019/01/01/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%91%E7%AB%A0-1/</link>
      <pubDate>Tue, 01 Jan 2019 00:01:00 +0900</pubDate>
      
      <guid>https://getumen.github.io/2019/01/01/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems-%EF%BC%91%E7%AB%A0-1/</guid>
      <description>&lt;h1 id=&#34;１章-導入&#34;&gt;１章 導入&lt;/h1&gt;

&lt;h2 id=&#34;マルチアームバンディット問題とは&#34;&gt;マルチアームバンディット問題とは&lt;/h2&gt;

&lt;p&gt;マルチアームバンディット問題は行動の候補からそのラウンドに行う行動を繰り返し割り当てる問題である．
各ラウンドで，１つの行動を割り当て，報酬を受け取る．
このときのゴールは報酬の合計が最大になるように繰り返し行動を割り当てることである．
バンディットという名前はアメリカのスロットマシンのスラングに由来している．
カジノで，プレイヤーがたくさんのスロットマシンからアームを引くスロットマシンを次々に選んで，コインを入れていく問題がマルチアームバンディット問題となる．&lt;/p&gt;

&lt;p&gt;バンディット問題は限られた情報の中で繰り返し意思決定していく問題である．そして，この問題は繰り返しの試行の中で，過去にうまく行ったアームを引き続けるのか，今後より高い報酬を返すかもしれないアームを探すのかという探索と活用のトレードオフに直面する．&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</title>
      <link>https://getumen.github.io/2019/01/01/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0900</pubDate>
      
      <guid>https://getumen.github.io/2019/01/01/regret-analysis-of-stochastic-and-nonstochastic-multi-armed-bandit-problems/</guid>
      <description>&lt;h1 id=&#34;このページについて&#34;&gt;このページについて&lt;/h1&gt;

&lt;p&gt;バンディット問題に対するアルゴリズムについてのサーベイ論文である&lt;a href=&#34;http://sbubeck.com/SurveyBCB12.pdf&#34;&gt;Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems&lt;/a&gt;を読んでまとめる．&lt;br /&gt;
このページでは概要の全訳と他の投稿のリンクを貼る．&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>